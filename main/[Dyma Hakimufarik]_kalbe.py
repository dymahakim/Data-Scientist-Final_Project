# -*- coding: utf-8 -*-
"""final project Rakamin.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rywLN1Lsl83z45Vtd-u5D0BVDPGrDLGH
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error

"""#DATA INGESTION

##DATA EXTRACTION
"""

dfcustomer = pd.read_csv('/content/Case Study - Customer.csv', delimiter = ";", decimal = ",")
dfproduct = pd.read_csv('/content/Case Study - Product.csv', delimiter = ";", decimal = ",")
dfstore = pd.read_csv('/content/Case Study - Store.csv', delimiter = ";", decimal = ",")
dftransaction = pd.read_csv('/content/Case Study - Transaction.csv', delimiter = ";", decimal = ",")

"""##DATA TRANSFORMATION"""

dfcustomer.head()

dfproduct.head()

dfstore.head()

dftransaction.head()

dfcustomer.info()

dfcustomer[dfcustomer.isnull().any(axis=1)]

dfcustomer_modus = dfcustomer[(dfcustomer['Age'] >= 27) & (dfcustomer['Age'] <= 33)]

dfcustomer_modus["Marital Status"].value_counts()

value_nan = "Married"
dfcustomer['Marital Status'] = dfcustomer['Marital Status'].fillna(value_nan)

dfcustomer[dfcustomer["CustomerID"].isin([443, 416, 10])]

dfproduct.info()

dfstore.info()

dftransaction.info()

"""Data Merge for machine learning"""

from datetime import datetime
dftransaction['Date'] = pd.to_datetime(dftransaction['Date'],format='%d/%m/%Y')
dftransaction.head()

dfpredict = dftransaction.groupby('Date').agg(total_qty=('Qty', 'sum'))
dfpredict

dfpredict.columns

dfclustering_merge1 = pd.merge(dftransaction, dfcustomer, on='CustomerID')
dfclustering_merge2 = pd.merge(dfclustering_merge1, dfstore, on='StoreID')
dfcluster = pd.merge(dfclustering_merge2, dfproduct, on='ProductID')
dfcluster

dfbytransaction = dfcluster.groupby('CustomerID').agg(Transaction_count =('TransactionID', 'size'), Qty_sum = ('Qty', 'sum'), Total_amount = ('TotalAmount', 'sum'), Age =('Age', 'mean'))

dfbytransaction

"""#Machine Learning"""

! pip install pmdarima

from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.stattools import adfuller
from pmdarima import auto_arima
from statsmodels.tsa.arima.model import ARIMA
from pandas.plotting import autocorrelation_plot
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error

from statsmodels.tsa.stattools import adfuller
result = adfuller(dfpredict['total_qty'])
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])
print('Critical Values:')
for key, value in result[4].items():
  print('\t%s: %.3f' % (key, value))

dfpredict.plot()
plt.show()

decompose = seasonal_decompose(dfpredict)

fig,ax = plt.subplots(3,1,figsize=(15,12))
decompose.trend.plot(ax=ax[0])
ax[0].set_title('Trend')
decompose.seasonal.plot(ax=ax[1])
ax[1].set_title('Seasonal')
decompose.resid.plot(ax=ax[2])
ax[2].set_title('Residual')

plt.tight_layout()
plt.show()

# Plot ACF using default method
plot_acf(dfpredict, lags=30)
plt.xlabel('Lag')
plt.ylabel('Autocorrelation')
plt.title('Autocorrelation Function (ACF)')
plt.show()

# Plot PACF using ywm method
plot_pacf(dfpredict, lags=30,  method='ywm')
plt.xlabel('Lag')
plt.ylabel('Partial Autocorrelation')
plt.title('Partial Autocorrelation Function (PACF)')
plt.show()

# Split the data into training and testing sets
cut_off = round(dfpredict.shape[0] * 0.8)
df_train = dfpredict[:cut_off]
df_test = dfpredict[cut_off:]

# Check the shapes of the train and test sets
print("Training set shape:", df_train.shape)
print("Testing set shape:", df_test.shape)

# check which ARIMA models best suit using auto ARIMA
auto_arima_model = auto_arima(df_train['total_qty'], seasonal=False, stepwise=False, suppress_warnings=True, trace = True)
auto_arima_model.summary()

# Function to calculate RMSE
def rmse(y_actual, y_pred):
  print(f'RMSE value {mean_squared_error(y_actual, y_pred)**0.5}')

# Function to eval machine learning modelling
def eval(y_actual, y_pred):
  print(f'MAE value {mean_absolute_error(y_actual, y_pred)}')

from statsmodels.tsa.statespace.sarimax import SARIMAX

# ARIMA Modelling
y = df_train['total_qty']
model = ARIMA(y, order = (1, 0, 1))
model = model.fit()

# Model Prediction
pred = model.get_forecast(len(df_test))
pred_df = pred.conf_int()
pred_df['predictions'] = model.predict(start = pred_df.index[0], end = pred_df.index[-1])
pred_df.index = df_test.index
pred_out = pred_df['predictions']

# Evaluate the model
rmse(df_test['total_qty'], pred_out)
eval(df_test['total_qty'], pred_out)

# Plot
plt.figure(figsize=(20, 5))
plt.plot(df_train['total_qty'], label='Data Train')
plt.plot(df_test['total_qty'], color='red', label='Data Test')
plt.plot(pred_out, color='black', label='ARIMA Prediction')
plt.legend()

# Plot and Visualize Predicted Values for Quantity Sold
plt.figure(figsize=(10, 6))
plt.plot(pred_out, color='black', label='ARIMA Prediction')
plt.title('Quantity Sold Forecast')
plt.legend()
plt.show()

"""## CLUSTERING COSTUMER"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn import preprocessing

dfcluster.head()

# Identified columns with high correlation
dfcluster.corr()

dfc1 = dfcluster.groupby('CustomerID').agg(Transaction_count =('TransactionID', 'size'), Total_amount = ('TotalAmount', 'sum'), Age =('Age', 'mean'))
dfc1

# create an empty list to store the inertia values
inertia = []

# create a range of k values to test
k_range = range(1, 11)

# fit KMeans for each k value and append the inertia to the list
for k in k_range:
    model = KMeans(n_clusters=k, n_init='auto')
    model.fit(dfc1)
    inertia.append(model.inertia_)

# plot the inertia values against k values
plt.plot(k_range, inertia, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.show()

model_kmeans = KMeans(n_clusters=3)

x = dfc1[["Total_amount","Age"]]

model_kmeans.fit(x)

model_kmeans.labels_

dfc1['cluster'] = model_kmeans.labels_

dfc1

dfc1[dfc1['cluster'] == 0]

dfc1[dfc1['cluster'] == 0].describe()

dfc1[dfc1['cluster'] == 1]

dfc1[dfc1['cluster'] == 1].describe()

dfc1[dfc1['cluster'] == 2]

dfc1[dfc1['cluster'] == 2].describe()

# Convert 'cluster' column to categorical data type
dfc1['cluster'] = dfc1['cluster'].astype('category')

# Create the scatter plot using Seaborn
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Age', y='Total_amount', data=dfc1, hue='cluster', palette='Set1', s=70)
plt.xlabel('Quantity')
plt.ylabel('TotalAmount')
plt.title('KMeans Clustering Customer Segmentation')
plt.legend(title='Cluster')
plt.show()